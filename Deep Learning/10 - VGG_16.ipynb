{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('Datasets/dogsvscats/train.hdf5') as f:\n",
    "    X_train = f['X'][:2000]\n",
    "    y_train = f['y'][:2000]\n",
    "with h5py.File('Datasets/dogsvscats/test.hdf5') as f:\n",
    "    X_test = f['X'][:]\n",
    "    y_test = f['y'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 150, 150, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1250, 150, 150, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1250,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OneHotEncoder(sparse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneHotEncoder(categorical_features='all', dtype=<class 'numpy.float64'>,\n",
       "       handle_unknown='error', n_values='auto', sparse=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.fit(y_train.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = encoder.transform(y_train.reshape(-1, 1))\n",
    "y_test = encoder.transform(y_test.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_batch(X, y, size):\n",
    "    m = X.shape[0]\n",
    "    index = np.random.permutation(np.arange(m))\n",
    "    for i in range(m//size):\n",
    "        batch_i = index[i*size: i*size+size]\n",
    "        yield X[batch_i], y[batch_i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_layer(X, channels, strides=[1, 1, 1, 1], kernel_initializer=None, reuse=False, name=\"conv\"):\n",
    "    with tf.variable_scope(\"Var_\"+name, reuse=reuse):\n",
    "        \n",
    "        if kernel_initializer:\n",
    "            initializer = kernel_initializer()\n",
    "        else:\n",
    "            initializer = tf.random_normal_initializer(stddev=0.01)\n",
    "        \n",
    "        shape = [3, 3, X.shape[3].value, channels]\n",
    "        \n",
    "        if reuse:\n",
    "            W = tf.get_variable('W')\n",
    "            b = tf.get_variable('B')\n",
    "        else:\n",
    "            W = tf.get_variable(shape=shape, initializer=initializer, name='W')\n",
    "            b = tf.get_variable(initializer=tf.constant(0.1, shape=[channels]), name='B')\n",
    "\n",
    "        conv = tf.nn.conv2d(X, W, strides=strides, padding=\"SAME\")\n",
    "        act = tf.nn.relu(conv + b)\n",
    "        \n",
    "        tf.summary.histogram(\"weights\", W)\n",
    "        tf.summary.histogram(\"biases\", b)\n",
    "        tf.summary.histogram(\"activations\", act)\n",
    "        \n",
    "        return act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fc_layer(X, units, activation=None, kernel_initializer=None, regularizer=None, reuse=False, name='FC'):\n",
    "    \n",
    "    with tf.variable_scope(\"Var_\"+name, reuse=reuse):\n",
    "\n",
    "        if kernel_initializer:\n",
    "            initializer = kernel_initializer()\n",
    "        else:\n",
    "            initializer = tf.random_normal_initializer(stddev=0.01)\n",
    "\n",
    "        if reuse:\n",
    "            W = tf.get_variable('W')\n",
    "            b = tf.get_variable('B')\n",
    "        else:\n",
    "            W = tf.get_variable(shape=[X.shape[1].value, units], initializer=initializer, regularizer=regularizer, name='W')\n",
    "            b = tf.get_variable(initializer=tf.constant(0.1, shape=[units]), name='B')\n",
    "\n",
    "        Z = tf.add(tf.matmul(X, W), b)\n",
    "\n",
    "        if activation:\n",
    "            A = activation(Z)\n",
    "        else:\n",
    "            A = Z\n",
    "\n",
    "        tf.summary.histogram(\"weights\", W)\n",
    "        tf.summary.histogram(\"biases\", b)\n",
    "        tf.summary.histogram(\"activations\", A)\n",
    "\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_model(X, y, regularization_rate=0.0, drop_rate=0.0, reuse=False, is_training=True, name=\"Conv_model\"):\n",
    "    with tf.name_scope(name):\n",
    "        regularizer = tf.contrib.layers.l2_regularizer(scale=regularization_rate)\n",
    "        kernel_initializer = tf.contrib.layers.xavier_initializer\n",
    "        activation = tf.nn.relu\n",
    "        \n",
    "        X = conv_layer(X, channels=64, kernel_initializer=kernel_initializer, reuse=reuse, name=f\"conv_a1\")\n",
    "        X = conv_layer(X, channels=64, kernel_initializer=kernel_initializer, reuse=reuse, name=f\"conv_a2\")\n",
    "        X = tf.nn.max_pool(X, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"SAME\")\n",
    "\n",
    "        X = conv_layer(X, channels=128, kernel_initializer=kernel_initializer, reuse=reuse, name=f\"conv_b1\")\n",
    "        X = conv_layer(X, channels=128, kernel_initializer=kernel_initializer, reuse=reuse, name=f\"conv_b2\")\n",
    "        X = tf.nn.max_pool(X, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"SAME\")\n",
    "\n",
    "        X = conv_layer(X, channels=256, kernel_initializer=kernel_initializer, reuse=reuse, name=f\"conv_c1\")\n",
    "        X = conv_layer(X, channels=256, kernel_initializer=kernel_initializer, reuse=reuse, name=f\"conv_c2\")\n",
    "        X = conv_layer(X, channels=256, kernel_initializer=kernel_initializer, reuse=reuse, name=f\"conv_c3\")\n",
    "        X = tf.nn.max_pool(X, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"SAME\")\n",
    "\n",
    "        X = conv_layer(X, channels=512, kernel_initializer=kernel_initializer, reuse=reuse, name=f\"conv_d1\")\n",
    "        X = conv_layer(X, channels=512, kernel_initializer=kernel_initializer, reuse=reuse, name=f\"conv_d2\")\n",
    "        X = conv_layer(X, channels=512, kernel_initializer=kernel_initializer, reuse=reuse, name=f\"conv_d3\")\n",
    "        X = tf.nn.max_pool(X, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"SAME\") \n",
    "\n",
    "        X = conv_layer(X, channels=512, kernel_initializer=kernel_initializer, reuse=reuse, name=f\"conv_e1\")\n",
    "        X = conv_layer(X, channels=512, kernel_initializer=kernel_initializer, reuse=reuse, name=f\"conv_e2\")\n",
    "        X = conv_layer(X, channels=512, kernel_initializer=kernel_initializer, reuse=reuse, name=f\"conv_e3\")\n",
    "        X = tf.nn.max_pool(X, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"SAME\")  \n",
    "        \n",
    "        dims = np.prod(X.shape.as_list()[1:])\n",
    "        X = tf.reshape(X, [-1, dims])\n",
    "        \n",
    "        X = fc_layer(X, 4096, activation, kernel_initializer, regularizer, reuse=reuse, name=f\"fc_1\")\n",
    "        X = tf.layers.dropout(X, rate=drop_rate, training=is_training)\n",
    "        X = fc_layer(X, 4096, activation, kernel_initializer, regularizer, reuse=reuse, name=f\"fc_2\")\n",
    "        X = tf.layers.dropout(X, rate=drop_rate, training=is_training)\n",
    "        \n",
    "        # In the original VGG the labels had 1000 classes, here is just binary classification\n",
    "        output = fc_layer(X, y.shape[1].value, kernel_initializer=kernel_initializer, regularizer=regularizer, reuse=reuse, name=f\"last_fc\" )\n",
    "        tf.summary.histogram('logits', output)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_model(X_train, y_train, learning_rate, epochs, batch_size,\n",
    "                l2_rate, drop_rate, logdir, X_test, y_test, print_acc=False):\n",
    "    tf.reset_default_graph()\n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        n_features = X_train.shape[1]\n",
    "        n_labels = y_train.shape[1]\n",
    "        \n",
    "        X = tf.placeholder(tf.float32, shape=[None, 150, 150, 3], name=\"X\")\n",
    "        paddings = tf.constant([[0, 0], [5, 5], [5, 5], [0, 0]]) \n",
    "        X_image = tf.pad(X, paddings)#I'm padding the image to be 160*160 so after 5 maxpooling it will be 5*5(160/2^5)\n",
    "        y = tf.placeholder(tf.float32, shape=[None, n_labels], name=\"labels\")\n",
    "\n",
    "\n",
    "        train_logits = conv_model(X_image, y, regularization_rate=l2_rate, drop_rate=drop_rate, reuse=False, is_training=True, name=\"Conv_Model\")\n",
    "        test_logits = conv_model(X_image, y, reuse=True, is_training=False, name=\"Conv_Eval_Model\")\n",
    "\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "                                  logits=train_logits, labels=y), name=\"loss\")\n",
    "            tf.summary.scalar(\"loss\", loss)\n",
    "\n",
    "        with tf.name_scope(\"train\"):\n",
    "            train_step = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "        with tf.name_scope(\"test_accuracy\"):\n",
    "            correct_prediction = tf.equal(tf.argmax(test_logits, 1), tf.argmax(y, 1))\n",
    "            test_accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "            tf.summary.scalar(\"test_accuracy\", test_accuracy)\n",
    "        \n",
    "        with tf.name_scope(\"train_accuracy\"):\n",
    "            correct_prediction = tf.equal(tf.argmax(train_logits, 1), tf.argmax(y, 1))\n",
    "            train_accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "            tf.summary.scalar(\"train_accuracy\", train_accuracy)\n",
    "\n",
    "        summ = tf.summary.merge_all()\n",
    "\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        writer = tf.summary.FileWriter(f\"./summary/{logdir}\")\n",
    "        writer.add_graph(sess.graph)\n",
    "\n",
    "        for epoch in range(1, epochs+1):\n",
    "            for X_batch, y_batch in get_next_batch(X_train, y_train, batch_size):\n",
    "                sess.run(train_step, {X: X_batch, y: y_batch})\n",
    "                \n",
    "            if print_acc and epoch%10 == 0:\n",
    "                train_sample = get_next_batch(X_train, y_train, 32).__next__()\n",
    "                test_sample = get_next_batch(X_test, y_test, 32).__next__()\n",
    "                \n",
    "                test_acc, s = sess.run([test_accuracy, summ],  {X: test_sample[0], y: test_sample[1]})\n",
    "                train_l, train_acc = sess.run([loss, train_accuracy], {X: train_sample[0], y: train_sample[1]})\n",
    "                print(\"Epoch:\", np.round(epoch, 4),\n",
    "                      \"Train Loss:\", np.round(train_l, 4),\n",
    "                      \"Train accuracy:\", np.round(train_acc, 4),\n",
    "                      \"Test accuracy:\", np.round(test_acc,4))\n",
    "                writer.add_summary(s, epoch)\n",
    "                saver.save(sess, f\"./model/{logdir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "Epoch: 10 Train Loss: 0.7206 Train accuracy: 0.4375 Test accuracy: 0.4688\n",
      "Epoch: 20 Train Loss: 0.7306 Train accuracy: 0.5 Test accuracy: 0.5312\n",
      "Epoch: 30 Train Loss: 0.6406 Train accuracy: 0.625 Test accuracy: 0.5\n"
     ]
    }
   ],
   "source": [
    "mnist_model(X_train=X_train, y_train=y_train, \n",
    "            learning_rate=1e-5, epochs=300, batch_size=32, \n",
    "            l2_rate=0.0, drop_rate=0.5, logdir='VGG-16', X_test=X_test, y_test=y_test, print_acc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum([np.prod(v.get_shape().as_list()) for v in tf.trainable_variables()]) #this outputs the number of variables(parameters) of the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
