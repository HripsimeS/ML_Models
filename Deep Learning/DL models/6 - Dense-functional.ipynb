{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load('Datasets/Fashion_MNIST/X_train.npy')\n",
    "X_test = np.load('Datasets/Fashion_MNIST/X_test.npy')\n",
    "y_train = np.load('Datasets/Fashion_MNIST/y_train.npy')\n",
    "y_test = np.load('Datasets/Fashion_MNIST/y_test.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OneHotEncoder(sparse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneHotEncoder(categorical_features='all', dtype=<class 'numpy.float64'>,\n",
       "       handle_unknown='error', n_values='auto', sparse=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.fit(y_train.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = encoder.transform(y_train.reshape(-1, 1))\n",
    "y_test = encoder.transform(y_test.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_batch(X, y, size):\n",
    "    m = X.shape[0]\n",
    "    index = np.random.permutation(np.arange(m))\n",
    "    for i in range(m//size):\n",
    "        batch_i = index[i*size: i*size+size]\n",
    "        yield X[batch_i], y[batch_i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fc_layer(X, units, activation=None, kernel_initializer=None, regularizer=None, reuse=False, name='FC'):\n",
    "    \n",
    "    with tf.variable_scope(\"Var_\"+name, reuse=reuse):\n",
    "\n",
    "        if kernel_initializer:\n",
    "            initializer = kernel_initializer()\n",
    "        else:\n",
    "            initializer = tf.random_normal_initializer(stddev=0.01)\n",
    "\n",
    "        if reuse:\n",
    "            W = tf.get_variable('W')\n",
    "            b = tf.get_variable('B')\n",
    "        else:\n",
    "            W = tf.get_variable(shape=[X.shape[1].value, units], initializer=initializer, regularizer=regularizer, name='W')\n",
    "            b = tf.get_variable(initializer=tf.constant(0.1, shape=[units]), name='B')\n",
    "\n",
    "        Z = tf.add(tf.matmul(X, W), b)\n",
    "\n",
    "        if activation:\n",
    "            A = activation(Z)\n",
    "        else:\n",
    "            A = Z\n",
    "\n",
    "        tf.summary.histogram(\"weights\", W)\n",
    "        tf.summary.histogram(\"biases\", b)\n",
    "        tf.summary.histogram(\"activations\", A)\n",
    "\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dnn_model(X, y, layers, regularization_rate=0.0, drop_rate=0.0, reuse=False, is_training=True, name=\"DNN\"):\n",
    "    with tf.name_scope(name):    \n",
    "        regularizer = tf.contrib.layers.l2_regularizer(scale=regularization_rate)\n",
    "        kernel_initializer = tf.contrib.layers.xavier_initializer\n",
    "        activation = tf.nn.relu\n",
    "\n",
    "        for i, layer in enumerate(layers):\n",
    "            X = fc_layer(X, layer, activation, kernel_initializer, regularizer, reuse=reuse, name=f\"fc{i+1}\")\n",
    "            X = tf.layers.dropout(X, rate=drop_rate, training=is_training)\n",
    "\n",
    "        output = fc_layer(X, y.shape[1].value, kernel_initializer=kernel_initializer, regularizer=regularizer, reuse=reuse, name=f\"last_fc\" )\n",
    "        tf.summary.histogram('logits', output)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, y_train, layers, learning_rate, epochs, batch_size,\n",
    "          l2_rate, drop_rate, logdir, X_test, y_test, print_acc=False):\n",
    "    tf.reset_default_graph()\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        n_features = X_train.shape[1]\n",
    "        n_labels = y_train.shape[1]\n",
    "\n",
    "        X = tf.placeholder(tf.float32, shape=[None, n_features], name='features')\n",
    "        y = tf.placeholder(tf.float32, shape=[None, n_labels], name='labels')\n",
    "        \n",
    "        train_logits = dnn_model(X, y, layers, l2_rate, drop_rate, reuse=False, is_training=True, name=\"DNN\")\n",
    "        test_logits = dnn_model(X, y, layers, reuse=True, is_training=False, name=\"Testing_DNN\")\n",
    "        \n",
    "        with tf.name_scope(\"train_accuracy\"):\n",
    "            train_pred = tf.equal(tf.argmax(train_logits,1), tf.argmax(y,1))\n",
    "            train_accuracy = tf.reduce_mean(tf.cast(train_pred,tf.float32))\n",
    "            tf.summary.scalar('train_accuracy', train_accuracy)\n",
    "            \n",
    "        with tf.name_scope(\"test_accuracy\"):\n",
    "            test_pred = tf.equal(tf.argmax(test_logits,1), tf.argmax(y,1))\n",
    "            test_accuracy = tf.reduce_mean(tf.cast(test_pred,tf.float32))\n",
    "            tf.summary.scalar('test_accuracy', test_accuracy)\n",
    "        \n",
    "        with tf.name_scope(\"train_loss\"):\n",
    "            train_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y, logits=train_logits), name='train_loss')\n",
    "            tf.summary.scalar('train_loss', train_loss)\n",
    "            \n",
    "        with tf.name_scope(\"train\"):\n",
    "            train_op = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(train_loss)\n",
    "            \n",
    "        summ = tf.summary.merge_all()\n",
    "        \n",
    "        saver = tf.train.Saver()\n",
    "    \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        writer = tf.summary.FileWriter(f\"./summary/{logdir}\")\n",
    "        writer.add_graph(sess.graph)\n",
    "        \n",
    "        for epoch in range(1, epochs+1):\n",
    "            for X_batch, y_batch in get_next_batch(X_train, y_train, batch_size):\n",
    "                sess.run(train_op, {X: X_batch, y: y_batch})\n",
    "\n",
    "            if print_acc and epoch%10 == 0:\n",
    "                train_sample = get_next_batch(X_train, y_train, X_test.shape[0]).__next__()\n",
    "                \n",
    "                test_acc, s = sess.run([test_accuracy, summ], {X: X_test, y: y_test})\n",
    "                train_l, train_acc = sess.run([train_loss, train_accuracy], {X: train_sample[0], y: train_sample[1]})\n",
    "                \n",
    "                print(\"Epoch:\", np.round(epoch, 4),\n",
    "                      \"Train Loss:\", np.round(train_l, 4),\n",
    "                      \"Train accuracy:\", np.round(train_acc, 4),\n",
    "                      \"Test accuracy:\", np.round(test_acc,4))\n",
    "                writer.add_summary(s, epoch)\n",
    "                saver.save(sess, f\"./model/{logdir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterGrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_params = [[128, 64, 64], [256, 128, 64], [128, 64], [256, 128]]\n",
    "param_grid = [{'layers': layers_params,\n",
    "              'learning_rate':[0.001, 0.0001, 0.00001],\n",
    "              'batch_size':[32],\n",
    "              'l2_rate': [0.0],\n",
    "              'drop_rate': [0.05, 0.25, 0.1]},\n",
    "              {'layers': layers_params,\n",
    "              'learning_rate':[0.001, 0.0001, 0.00001],\n",
    "              'batch_size':[32],\n",
    "              'l2_rate': [100.0, 10.0, 0.1],\n",
    "              'drop_rate': [0.0]},]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for params in ParameterGrid(param_grid):\n",
    "    name = \",\".join([f\"{k}={v}\" for k, v in params.items()])\n",
    "    \n",
    "    print(\"\\n\\n\", name)\n",
    "    \n",
    "    model(X_train=X_train, y_train=y_train, epochs=200, \n",
    "      logdir=name, X_test=X_test, y_test=y_test, print_acc=True, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
